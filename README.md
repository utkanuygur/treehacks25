# Hackathon 2024 Project

Welcome to the Hackathon 2024 repository. This project demonstrates a range of capabilities including log analytics, vector search with InterSystems IRIS, web application demos, and data preprocessing.

## Project Structure

- **.env** – Environment configuration.
- **.gitignore** – Git ignore rules.
- **app.py** – Entry point for the main application.
- **Apps/** – Contains additional application demos (e.g., the [simpleFlask](Apps/simpleFlask/README.md) demo).
- **data/** – Contains datasets:
  - `paul_graham/` – Essays and text data.
  - `scotch_review.csv`
  - `state_of_the_union.txt`
- **demo/** – Jupyter notebooks demonstrating various features:
  - [iris_notebook_container.ipynb](demo/iris_notebook_container.ipynb)
  - [IRISDatabaseOperationsUsingSQL.ipynb](demo/IRISDatabaseOperationsUsingSQL.ipynb)
  - [langchain_demo.ipynb](demo/langchain_demo.ipynb)
  - [llama_demo.ipynb](demo/llama_demo.ipynb)
  - Plus demo notes in [README.md](demo/README.md) and [SQLSyntax.md](demo/SQLSyntax.md)
- **HDFS_v1/** – Contains the HDFS log dataset as well as the preprocessed files:
  - Raw log file: `HDFS.log`
  - Preprocessed data under [HDFS_v1/preprocessed/README.md](HDFS_v1/preprocessed/README.md)
- **install/** – Installation files including the InterSystems IRIS Python driver wheels.
- **iris-env/** – The Python virtual environment.
- **main.py** – Main script for processing HDFS logs.
- **mains.ipynb** – Notebook showcasing data loading and model training.
- **splitter.py** – Script to split the HDFS anomaly dataset and report TPR/TNR.
- **train.csv / val.csv / test.csv** – Dataset splits generated by `splitter.py`.
- **model.pth** – Pre-trained model weights.
- **requirements.txt** – Python dependencies.
- **send.py** – Additional script (e.g., for sending data or notifications).

## Quickstart

1. **Clone the repository:**

    ```Shell
    git clone https://github.com/your_username/hackathon-2024.git
    cd hackathon-2024
    ```

2. **Set up your environment:**

   Create and activate a virtual environment (or use iris-env):

    ```Shell
    python -m venv venv
    source venv/bin/activate  # On Windows use 'venv\Scripts\activate'
    ```

3. **Install dependencies:**

    ```Shell
    pip install -r requirements.txt
    ```

4. **Install the InterSystems IRIS driver:**

   Choose the appropriate command for your operating system (see [install](http://_vscodecontentref_/1) for details):

    - **MacOS:**
      ```Shell
      pip install ./install/intersystems_irispython-5.0.1-8026-cp38.cp39.cp310.cp311.cp312-cp38.cp39.cp310cp311cp312-macosx_10_9_universal2.whl
      ```

    - **Linux aarch64:**
      ```Shell
      pip install ./install/intersystems_irispython-5.0.1-8026-cp38.cp39.cp310.cp311cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
      ```

    - **Linux x86_64:**
      ```Shell
      pip install ./install/intersystems_irispython-5.0.1-8026-cp38.cp39.cp310cp311cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
      ```

5. **Run the demos:**

   - **IRIS Vector Search and Database Demos:**
     Open and run the notebooks in the [demo](http://_vscodecontentref_/2) folder using Jupyter Notebook or VSCode. For example:
     
     ```Shell
     jupyter notebook demo/langchain_demo.ipynb
     ```

   - **Web Application Demo:**
     Check out the simpleFlask demo for a quick Flask-based example.

6. **Data Processing:**

   Run [main.py](http://_vscodecontentref_/3) to process HDFS logs or [splitter.py](http://_vscodecontentref_/4) to split the anomaly label dataset:
   
    ```Shell
    python main.py
    python splitter.py
    ```

## Additional Information

- **HDFS Dataset:**  
  The HDFS log dataset is located in [HDFS_v1](http://_vscodecontentref_/5), specifically the preprocessed files in [preprocessed](http://_vscodecontentref_/6). For more details about the dataset, refer to the HDFS_v1 README.

- **Documentation and References:**  
  For more information on using IRIS for vector search, see the official [InterSystems Documentation](https://docs.intersystems.com/).

## License

This project is licensed under the MIT License. See the LICENSE file for details.

## Contributing

Contributions are welcome. Please open issues or pull requests as needed.