{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a7e9ec",
   "metadata": {},
   "source": [
    "# PyTorch Training and FastAPI Deployment Notebook\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "- Load and preprocess a CSV dataset\n",
    "- Split the dataset into training, validation, and test sets\n",
    "- Define a custom PyTorch Dataset and a neural network model\n",
    "- Train the model and save it (reporting metrics including true positive rate and true negative rate)\n",
    "- Write a FastAPI inference endpoint that loads the saved model and provides predictions\n",
    "\n",
    "After running the notebook, you can run the FastAPI server by executing:\n",
    "```bash\n",
    "uvicorn app:app --reload\n",
    "```\n",
    "in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fbfd9ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print('Libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fa30d",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess Data\n",
    "\n",
    "Assume that `data.csv` is in the current directory. We will:\n",
    "\n",
    "- Convert the `Type` column to numeric (filling missing values with 0)\n",
    "- Map the `Label` column so that `Success` becomes 1 and `Fail` becomes 0\n",
    "- Use all columns except `BlockId` and `Label` as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c60b0b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (575061, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Label</th>\n",
       "      <th>Type</th>\n",
       "      <th>E1</th>\n",
       "      <th>E2</th>\n",
       "      <th>E3</th>\n",
       "      <th>E4</th>\n",
       "      <th>E5</th>\n",
       "      <th>E6</th>\n",
       "      <th>E7</th>\n",
       "      <th>...</th>\n",
       "      <th>E20</th>\n",
       "      <th>E21</th>\n",
       "      <th>E22</th>\n",
       "      <th>E23</th>\n",
       "      <th>E24</th>\n",
       "      <th>E25</th>\n",
       "      <th>E26</th>\n",
       "      <th>E27</th>\n",
       "      <th>E28</th>\n",
       "      <th>E29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId  Label  Type  E1  E2   E3  E4  E5  E6  E7  ...  \\\n",
       "0  blk_-1608999687919862906      1   0.0   0   0  203   0  10   7   0  ...   \n",
       "1   blk_7503483334202473044      1   0.0   0   2    1   0   3   0   0  ...   \n",
       "2  blk_-3544583377289625738      0  21.0   0   0  203   0   3   0   0  ...   \n",
       "3  blk_-9073992586687739851      1   0.0   0   3    0   0   3   0   0  ...   \n",
       "4   blk_7854771516489510256      1   0.0   0   3    1  15   3   0   0  ...   \n",
       "\n",
       "   E20  E21  E22  E23  E24  E25  E26  E27  E28  E29  \n",
       "0    0   10    1   10    0    4   10    0    0    0  \n",
       "1    0    3    1    3    0    0    3    0    0    0  \n",
       "2    1    3    1    3    0    0    3    0    0    0  \n",
       "3    0    3    1    3    0    0    3    0    0    0  \n",
       "4    0    3    1    3    0    0    3    0    0    0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = 'HDFS_v1/preprocessed/event_occurrence_matrix.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocess the data\n",
    "df['Type'] = pd.to_numeric(df['Type'], errors='coerce').fillna(0)\n",
    "df['Label'] = df['Label'].apply(lambda x: 1 if x.strip().lower() == 'success' else 0)\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = [col for col in df.columns if col not in ['BlockId', 'Label']]\n",
    "target_col = 'Label'\n",
    "\n",
    "print('Data shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ddf1e",
   "metadata": {},
   "source": [
    "## Step 2: Split the Data and Save as CSVs\n",
    "\n",
    "We split the dataset into training (70%), validation (15%), and test (15%) sets and save them as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "21f4717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 402542, Validation samples: 86259, Test samples: 86260\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "df_train, df_temp = train_test_split(df, test_size=0.3, random_state=42, stratify=df[target_col])\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp[target_col])\n",
    "\n",
    "# Save splits as CSV\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_val.to_csv('val.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "\n",
    "print(f\"Train samples: {len(df_train)}, Validation samples: {len(df_val)}, Test samples: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b84cc",
   "metadata": {},
   "source": [
    "## Step 3: Define a Custom Dataset and Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1f96dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset and model classes defined.\n"
     ]
    }
   ],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, feature_cols, target_col):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.features = self.data[feature_cols].values.astype(np.float32)\n",
    "        self.labels = self.data[target_col].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx])\n",
    "        y = torch.tensor(self.labels[idx])\n",
    "        return x, y\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # Single logit output\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print('Custom Dataset and model classes defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b83fe",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model and Save It\n",
    "\n",
    "This cell trains the model for a few epochs and prints the training loss, validation loss, validation accuracy, true positive rate (TPR), and true negative rate (TNR). Finally, the trained model is saved to `model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "35606ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Train Loss: 0.1197, Val Loss: 0.0096, Val Acc: 99.68%, TPR: 99.92%, TNR: 91.49%\n",
      "Epoch [2/5] - Train Loss: 0.0040, Val Loss: 0.0018, Val Acc: 99.98%, TPR: 99.99%, TNR: 99.68%\n",
      "Epoch [3/5] - Train Loss: 0.0013, Val Loss: 0.0011, Val Acc: 99.98%, TPR: 99.98%, TNR: 99.80%\n",
      "Epoch [4/5] - Train Loss: 0.0010, Val Loss: 0.0010, Val Acc: 99.98%, TPR: 99.98%, TNR: 99.80%\n",
      "Epoch [5/5] - Train Loss: 0.0010, Val Loss: 0.0011, Val Acc: 99.99%, TPR: 99.99%, TNR: 99.80%\n",
      "Model saved as 'model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2048\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "input_dim = len(feature_cols)  # Number of features\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CSVDataset('train.csv', feature_cols, target_col)\n",
    "val_dataset = CSVDataset('val.csv', feature_cols, target_col)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = Net(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Expects raw logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features).squeeze(1)  # Squeeze to match labels shape\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * features.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    TP = TN = FP = FN = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            outputs = model(features).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * features.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            TP += ((preds == 1) & (labels == 1)).sum().item()\n",
    "            TN += ((preds == 0) & (labels == 0)).sum().item()\n",
    "            FP += ((preds == 1) & (labels == 0)).sum().item()\n",
    "            FN += ((preds == 0) & (labels == 1)).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = correct / total * 100.0\n",
    "    TPR = (TP / (TP + FN) * 100.0) if (TP + FN) > 0 else 0.0\n",
    "    TNR = (TN / (TN + FP) * 100.0) if (TN + FP) > 0 else 0.0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, TPR: {TPR:.2f}%, TNR: {TNR:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print(\"Model saved as 'model.pth'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d9773",
   "metadata": {},
   "source": [
    "## Step 5: Create a FastAPI Inference Endpoint\n",
    "\n",
    "The cell below writes a FastAPI application to a file named `app.py`. This app loads the saved model and exposes a `/predict` endpoint.\n",
    "\n",
    "To run the FastAPI server, execute the following in your terminal:\n",
    "```bash\n",
    "uvicorn app:app --reload\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e3b4b9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the same neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # Single logit\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set the input dimension (adjust this to match your features, e.g., if you have 30 features)\n",
    "input_dim = 30\n",
    "\n",
    "# Load the saved model\n",
    "model = Net(input_dim)\n",
    "model.load_state_dict(torch.load('model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "class PredictionInput(BaseModel):\n",
    "    features: list[float]  # List of feature values\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(input_data: PredictionInput):\n",
    "    # Convert the input list into a tensor and add a batch dimension\n",
    "    features_tensor = torch.tensor(input_data.features, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(features_tensor).squeeze(1)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "        predicted_class = 1 if probability > 0.5 else 0\n",
    "    return {\"probability\": probability, \"predicted_class\": predicted_class}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b9e9c8",
   "metadata": {},
   "source": [
    "The FastAPI app has now been written to `app.py`. To run the server, open a terminal in the notebook's directory and run:\n",
    "```bash\n",
    "uvicorn app:app --reload\n",
    "```\n",
    "\n",
    "You can then send a POST request to `http://127.0.0.1:8000/predict` with a JSON body containing a list of feature values. For example:\n",
    "\n",
    "```bash\n",
    "curl -X POST \"http://127.0.0.1:8000/predict\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"features\": [0, 3, 1, 15, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 0, 0, 3, 0, 0, 0, 0]}'\n",
    "```\n",
    "\n",
    "Make sure that the length of the `features` list exactly matches the `input_dim` (in this example, 30)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
